Title,Body,Labels,Milestone
Python project structure and pyproject.toml,"Set up standard Python project structure with src/ layout. Create pyproject.toml for dependencies (Poetry or setuptools). Add requirements.txt and setup.py. Configure package metadata (name, version, description, author). Set up __init__.py files for proper package imports.","type:task;area:infra;priority:P0",M1: Requirements & Design
Python environment management (venv/conda),"Document virtual environment setup process. Provide scripts for creating/activating venv (scripts/setup_env.sh or .ps1). Consider conda for data science dependencies if needed. Add .python-version file (for pyenv). Add .venv/ to .gitignore.","type:task;area:infra;priority:P1",M1: Requirements & Design
Python code formatting (Black/Ruff),"Configure Black for code formatting (100 char line length recommended). Add Ruff for fast linting (replaces flake8, isort, pyupgrade). Configure formatting in pyproject.toml. Add make format and make lint commands. Ensure CI runs these checks.","type:task;area:infra;priority:P1",M1: Requirements & Design
Type checking with mypy,"Set up mypy for static type checking. Add type hints to critical modules (extraction, clustering, database models). Configure mypy.ini or pyproject.toml with strict settings. Add make typecheck command. Aim for gradual typing (start with critical paths).","type:task;area:infra;priority:P2",M1: Requirements & Design
Pre-commit hooks for Python,"Configure pre-commit with Python hooks: black, ruff, mypy, trailing-whitespace, end-of-file-fixer, check-yaml. Add .pre-commit-config.yaml. Document installation: pre-commit install. Ensure hooks run before commit to catch issues early.","type:task;area:infra;priority:P1",M1: Requirements & Design
Python dependency management strategy,"Choose dependency management approach: Poetry (modern, lock file), pip-tools (compile requirements), or plain requirements.txt. Document dependency update process. Set up Dependabot or Renovate for automated security updates. Pin major versions in production.","type:task;area:infra;priority:P1",M1: Requirements & Design
Pytest configuration and fixtures,"Configure pytest with pytest.ini or pyproject.toml (testpaths, markers, addopts). Set up conftest.py with shared fixtures: mock conferences, DB session, mock LLM responses. Add pytest-cov for coverage reporting. Configure coverage thresholds (>80%). Add make test command.","type:task;area:infra;priority:P1",M1: Requirements & Design
Scraper: AI Deadlines (aideadlin.es),"Implement dedicated scraper for Papers With Code's AI Deadlines site (aideadlin.es). Parse conference list HTML/JSON API if available. Extract: conference name, acronym, deadlines (abstract, paper, notification), conference dates, location, link. Handle pagination/filtering by field (NLP, CV, ML). Store in knowledge base with source attribution.","type:feature;area:retrieval;priority:P0",M2: Data Source Integration & Web Crawling
Scraper: ACL Sponsored Events,"Parse ACL events page (https://www.aclweb.org/portal/upcoming). Extract event list: name, type (conference/workshop), date, location, CFP link. Handle both upcoming and past events (filter by date). ACL events are authoritative for NLP conferences—high priority source.","type:feature;area:retrieval;priority:P1",M2: Data Source Integration & Web Crawling
Scraper: ChairingTool platform,"Retrieve conference info from ChairingTool platform (used by many CS conferences). Check if API available; otherwise scrape known conference URLs (e.g., https://www.chairingtool.com/conferences/). Extract: conference metadata, important dates (submission, notification, camera-ready). Handle authentication if required.","type:feature;area:retrieval;priority:P1",M2: Data Source Integration & Web Crawling
Scraper: ELRA conference listings (LREC),"Parse ELRA website (http://www.elra.info/) for upcoming conferences like LREC (Language Resources and Evaluation Conference). Extract conference dates, submission deadlines, location. ELRA is key for language resources / corpus linguistics events.","type:feature;area:retrieval;priority:P2",M2: Data Source Integration & Web Crawling
Scraper: WikiCFP integration,"Implement WikiCFP scraper as fallback/supplementary source (http://www.wikicfp.com/cfp/). Parse CFP listings by category (AI, NLP, ML, etc.). Extract: event name, deadlines, location, link. Handle WikiCFP's HTML structure changes gracefully (structure is somewhat brittle). Use as backup when official sources unavailable.","type:feature;area:retrieval;priority:P2",M2: Data Source Integration & Web Crawling
PDF text extraction pipeline,"Integrate PDF parsing library: PyMuPDF (fast) or pdfplumber (better tables). Extract text from PDF CFPs when HTML not available. Handle multi-column layouts (common in CFPs). Implement table detection for important dates tables. Pass extracted text to NLP extraction pipeline (treat like HTML text).","type:feature;area:extraction;priority:P2",M3: Information Extraction Pipeline
PDF download and caching,"Download PDF CFPs when HTML unavailable (some conferences only publish PDF CFPs). Cache PDFs locally (temp dir) or S3 bucket to avoid re-downloading. Track PDF URL in database (pdf_url field). Implement cleanup policy for old PDFs (e.g., delete after 30 days or when conference passes).","type:feature;area:retrieval;priority:P2",M2: Data Source Integration & Web Crawling
LLM API setup and configuration,"Set up OpenAI API client (openai Python library) or Anthropic (anthropic library). Configure API keys via secrets management (from issue #23). Implement rate limiting: max requests/min, exponential backoff. Add error handling: timeout, API errors, invalid responses. Implement cost tracking: log tokens used per request. Consider using LiteLLM for multi-provider support.","type:task;area:extraction;priority:P0",M1: Requirements & Design
LangChain agent framework setup,"Install and configure LangChain library. Set up agent with tools: web search tool, extraction tool, date parsing tool. Implement agent memory/state management (conversation buffer or summary memory). Configure agent executor with max iterations and early stopping. Test basic agent workflow: given a conference name, find and extract its deadline.","type:feature;area:extraction;priority:P1",M3: Information Extraction Pipeline
Extraction prompt engineering and optimization,"Design and test prompts for conference field extraction. Define JSON schema for extraction output (name, deadlines, location, etc.). Include few-shot examples in prompt (show 2-3 examples of CFP text → JSON). Iterate on prompt templates based on extraction accuracy. A/B test prompt variations (compare accuracy on test set). Document best-performing prompts.","type:task;area:extraction;priority:P1",M3: Information Extraction Pipeline
Ground truth dataset curation,"Manually curate ground truth dataset of 10-20 conferences with verified data. For each conference, record: official source URL, correct deadlines (with timezone), location, dates. Document sources (screenshot or archive). Use this dataset for evaluation baseline (calculate precision/recall). Update dataset periodically as new conferences announced.","type:task;area:infra;priority:P1",M6: Evaluation & Iteration
Extraction accuracy metrics (Precision/Recall),"Implement field-level precision/recall calculation. For each field (submission_deadline, abstract_deadline, notification_date, conference_dates, location): compare extracted vs ground truth. Report per-field metrics: precision (correct extractions / total extractions), recall (correct extractions / total ground truth). Generate accuracy report: overall score, per-field breakdown, failure analysis. Target: >90% accuracy on key dates.","type:task;area:infra;priority:P1",M6: Evaluation & Iteration
Change detection latency tracking,"Track time between official deadline change and ConfRadar detection. Measure: (1) Time from official site update to next crawl, (2) Time from crawl to extraction/DB update, (3) Time from DB update to output sync. Set alerting threshold: warn if data is stale (>24 hours since last update for a conference). Report latency distribution: p50, p95, p99.","type:task;area:infra;priority:P2",M6: Evaluation & Iteration
PostgreSQL schema design documentation,"Document detailed PostgreSQL schema: (1) ConferenceSeries table (id, name, acronym, aliases, field), (2) ConferenceEvent table (id, series_id, year, deadlines, location, dates, source_url, last_updated), (3) ChangeLog table (id, event_id, field_name, old_value, new_value, changed_at). Define indexes: (series_id, year), deadlines for sorting, last_updated for staleness checks. Define foreign keys, constraints (year between 2020-2030, deadlines < conference dates). Add ER diagram (ERD) using tool like dbdiagram.io or Mermaid.","type:docs;area:kb;priority:P1",M1: Requirements & Design
SQLAlchemy ORM models,"Implement SQLAlchemy models for ConferenceSeries, ConferenceEvent, ChangeLog. Define relationships: ConferenceEvent.series (foreign key to ConferenceSeries), ConferenceEvent.changes (one-to-many to ChangeLog). Add cascade deletes (if series deleted, cascade to events). Use declarative base. Add model methods: to_dict(), from_dict(). Create DB session management utilities: get_session(), create_tables(), drop_tables(). Add Alembic for migrations.","type:task;area:kb;priority:P0",M2: Data Source Integration & Web Crawling
BeautifulSoup/Scrapy implementation,"Implement HTML parsers using BeautifulSoup (bs4 library) for simple pages. Use CSS selectors (soup.select()) or XPath for robust parsing. For complex crawling (many pages, queues, rate limiting), consider Scrapy framework. Add CSS selector tests: if selector returns empty, log warning (page structure changed). Implement fallback selectors (multiple ways to find same element). Handle HTML encoding issues (chardet library).","type:task;area:retrieval;priority:P1",M2: Data Source Integration & Web Crawling
