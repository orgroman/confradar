# Development Guide

This guide covers development workflows for the ConfRadar monorepo with uv and the LiteLLM proxy.

## Environment

- Python: 3.10+
- Dependency manager: uv
- Database: PostgreSQL (Docker) or SQLite (tests only)
- Package: `packages/confradar`

## Frontend (Next.js) Development

We provide a Dockerized Next.js frontend located in `web/`.

- Dev server runs on http://localhost:3100
- Hot reload is enabled via bind mounts and polling watchers
- Uses Node 20 Alpine base image

### Start the frontend only

```powershell
# From repo root
docker compose up -d web

# View logs
docker compose logs -f web
```

### Start all services (including frontend)

```powershell
docker compose up -d
```

### Notes

- The Dagster UI uses port 3000; the frontend uses port 3100 to avoid conflicts.
- If file changes don't trigger hot reload, ensure polling is enabled (it's set in docker-compose by default).
- To rebuild the image after dependency changes (package.json):

```powershell
docker compose build web; docker compose up -d web
```

## Secret Management (Azure Key Vault)

All secrets (API keys, credentials) are managed in **Azure Key Vault** (`kvconfradar`).

**Azure Subscription ID:** `8592e500-3312-4991-9d2a-2b97e43b1810`

- Access secrets using Azure MCP for local development and CI/CD.
- OpenAI API key and Vercel v0 API key are stored in the vault.
- If needed, sync secrets from Azure Key Vault to GitHub repository secrets for automation.
- Never commit secrets to the repository.

**How to access secrets:**
1. Authenticate with Azure MCP to access `kvconfradar`.
2. Retrieve secrets for your local `.env` or CI/CD workflows.
3. For frontend (Vercel) development, use the Vercel v0 API key from the vault.
4. For LLM/OpenAI, use the OpenAI API key from the vault.

**Best Practices:**
- Use Azure Key Vault as the single source of truth for all secrets.
- Only sync secrets to GitHub if required for automation.
- Rotate secrets regularly and audit access permissions.

```powershell
uv sync --extra dev
cd packages/confradar
uv run pytest -q
```

## Database Development

### Connection Strings

Production (default):
```powershell
DATABASE_URL=postgresql+psycopg://confradar:confradar@localhost:5432/confradar
```

Testing (SQLite):
```powershell
$env:DATABASE_URL = "sqlite:///test.db"
```

Docker internal (for containers):
```powershell
DATABASE_URL=postgresql+psycopg://confradar:confradar@postgres:5432/confradar
```

### Working with PostgreSQL

```powershell
# Start PostgreSQL
docker compose up -d postgres

# Connect via psql
docker compose exec postgres psql -U confradar -d confradar

# Common queries
docker compose exec postgres psql -U confradar -d confradar -c "\dt"  # List tables
docker compose exec postgres psql -U confradar -d confradar -c "\d conferences"  # Describe table
docker compose exec postgres psql -U confradar -d confradar -c "SELECT COUNT(*) FROM conferences;"
```

### Backup and Restore

```powershell
# Backup database
docker compose exec postgres pg_dump -U confradar confradar > backup.sql

# Restore database
docker compose exec -T postgres psql -U confradar -d confradar < backup.sql
```

## LLM configuration

- API key: `CONFRADAR_SA_OPENAI` (preferred) or `OPENAI_API_KEY`
- Base URL (OpenAI-compatible): defaults to `http://localhost:4000`
- Override via: `LITELLM_BASE_URL`, `LLM_BASE_URL`, or `OPENAI_BASE_URL`
- Local proxy: `docker compose up -d` (from repo root)

## Coding standards

- Format: black
- Lint: ruff
- Tests: pytest (+ pytest-cov)

```powershell
cd packages/confradar
uv run ruff check
uv run black --check src tests
uv run pytest -q
```

## CI

GitHub Actions runs uv sync and tests on Windows and Ubuntu for pull requests.

## Notes

- Keep public APIs stable; add tests when changing behavior
- Prefer provider-agnostic LLM usage via LiteLLM (client or proxy)
- No real LLM calls in unit tests; mock LiteLLM

## Database migrations (Alembic)

We use Alembic for schema migrations. Migrations run against PostgreSQL by default.

```powershell
# Generate a new migration (autogenerate from models)
uv run alembic revision --autogenerate -m "<message>"

# Apply migrations
uv run alembic upgrade head

# Downgrade one revision
uv run alembic downgrade -1

# View migration history
uv run alembic history

# Test migration with SQLite
$env:DATABASE_URL = "sqlite:///test.db"
uv run alembic upgrade head
```

### Migration Best Practices

- Always review autogenerated migrations before committing
- Test migrations on a copy of production data when possible
- Keep migrations small and focused
- Include both upgrade and downgrade logic
- Never edit applied migrations (create new ones instead)

## Dagster Development

### Running Dagster locally

```powershell
cd packages/confradar

# Start webserver (UI on port 3000)
uv run dagster-webserver

# Start daemon (for schedules)
uv run dagster-daemon run
```

### Testing Dagster assets

```powershell
# Run Dagster tests
uv run pytest tests/test_dagster.py -v

# Run with integration tests (requires network)
uv run pytest tests/test_dagster.py -v -m integration
```

### Adding new scraper assets

1. Create spider in `src/confradar/scrapers/spiders/`
2. Add asset function in `src/confradar/dagster/assets/scrapers.py`
3. Import and add to `Definitions` in `src/confradar/dagster/definitions.py`
4. Add parameter to `store_conferences` in `src/confradar/dagster/assets/storage.py`
5. Add test to verify asset exists

See [Dagster Orchestration](Dagster-Orchestration) for detailed guide.

### Materializing assets

```powershell
cd packages/confradar

# Materialize all
uv run dagster asset materialize --select '*'

# Materialize specific asset
uv run dagster asset materialize --select 'ai_deadlines_conferences'
```