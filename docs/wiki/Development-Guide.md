# Development Guide

This guide covers development workflows for the ConfRadar monorepo with uv and the LiteLLM proxy.

## Environment

- Python: 3.10+
- Dependency manager: uv
- Database: PostgreSQL (Docker) or SQLite (tests only)
- Package: `packages/confradar`

```powershell
uv sync --extra dev
cd packages/confradar
uv run pytest -q
```

## Database Development

### Connection Strings

Production (default):
```powershell
DATABASE_URL=postgresql+psycopg://confradar:confradar@localhost:5432/confradar
```

Testing (SQLite):
```powershell
$env:DATABASE_URL = "sqlite:///test.db"
```

Docker internal (for containers):
```powershell
DATABASE_URL=postgresql+psycopg://confradar:confradar@postgres:5432/confradar
```

### Working with PostgreSQL

```powershell
# Start PostgreSQL
docker compose up -d postgres

# Connect via psql
docker compose exec postgres psql -U confradar -d confradar

# Common queries
docker compose exec postgres psql -U confradar -d confradar -c "\dt"  # List tables
docker compose exec postgres psql -U confradar -d confradar -c "\d conferences"  # Describe table
docker compose exec postgres psql -U confradar -d confradar -c "SELECT COUNT(*) FROM conferences;"
```

### Backup and Restore

```powershell
# Backup database
docker compose exec postgres pg_dump -U confradar confradar > backup.sql

# Restore database
docker compose exec -T postgres psql -U confradar -d confradar < backup.sql
```

## LLM configuration

- API key: `CONFRADAR_SA_OPENAI` (preferred) or `OPENAI_API_KEY`
- Base URL (OpenAI-compatible): defaults to `http://localhost:4000`
- Override via: `LITELLM_BASE_URL`, `LLM_BASE_URL`, or `OPENAI_BASE_URL`
- Local proxy: `docker compose up -d` (from repo root)

## Coding standards

- Format: black
- Lint: ruff
- Tests: pytest (+ pytest-cov)

```powershell
cd packages/confradar
uv run ruff check
uv run black --check src tests
uv run pytest -q
```

## CI

GitHub Actions runs uv sync and tests on Windows and Ubuntu for pull requests.

## Notes

- Keep public APIs stable; add tests when changing behavior
- Prefer provider-agnostic LLM usage via LiteLLM (client or proxy)
- No real LLM calls in unit tests; mock LiteLLM

## Database migrations (Alembic)

We use Alembic for schema migrations. Migrations run against PostgreSQL by default.

```powershell
# Generate a new migration (autogenerate from models)
uv run alembic revision --autogenerate -m "<message>"

# Apply migrations
uv run alembic upgrade head

# Downgrade one revision
uv run alembic downgrade -1

# View migration history
uv run alembic history

# Test migration with SQLite
$env:DATABASE_URL = "sqlite:///test.db"
uv run alembic upgrade head
```

### Migration Best Practices

- Always review autogenerated migrations before committing
- Test migrations on a copy of production data when possible
- Keep migrations small and focused
- Include both upgrade and downgrade logic
- Never edit applied migrations (create new ones instead)

## Dagster Development

### Running Dagster locally

```powershell
cd packages/confradar

# Start webserver (UI on port 3000)
uv run dagster-webserver

# Start daemon (for schedules)
uv run dagster-daemon run
```

### Testing Dagster assets

```powershell
# Run Dagster tests
uv run pytest tests/test_dagster.py -v

# Run with integration tests (requires network)
uv run pytest tests/test_dagster.py -v -m integration
```

### Adding new scraper assets

1. Create spider in `src/confradar/scrapers/spiders/`
2. Add asset function in `src/confradar/dagster/assets/scrapers.py`
3. Import and add to `Definitions` in `src/confradar/dagster/definitions.py`
4. Add parameter to `store_conferences` in `src/confradar/dagster/assets/storage.py`
5. Add test to verify asset exists

See [Dagster Orchestration](Dagster-Orchestration) for detailed guide.

### Materializing assets

```powershell
cd packages/confradar

# Materialize all
uv run dagster asset materialize --select '*'

# Materialize specific asset
uv run dagster asset materialize --select 'ai_deadlines_conferences'
```